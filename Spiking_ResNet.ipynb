{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gXhZCsxOxTGo",
      "metadata": {
        "id": "gXhZCsxOxTGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d103ae9-f2d6-4ae6-de4a-a1a5b3cad84e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WOdLOMWdxwgQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOdLOMWdxwgQ",
        "outputId": "627bf953-80bc-4745-8965-01b8b8c28be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gx6YDqDBhPNU",
      "metadata": {
        "id": "gx6YDqDBhPNU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import snntorch as snn\n",
        "import snntorch.spikegen as spikegen\n",
        "import snntorch.surrogate as surrogate\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z7SkNougvBXK",
      "metadata": {
        "id": "Z7SkNougvBXK"
      },
      "outputs": [],
      "source": [
        "# Set a fixed random seed\n",
        "RANDOM_SEED = 42  # You can choose any integer\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)  # for multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# When creating DataLoader, add worker_init_fn for reproducibility\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = RANDOM_SEED + worker_id\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g = g.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42aa9cd6",
      "metadata": {
        "id": "42aa9cd6"
      },
      "outputs": [],
      "source": [
        "def get_data_splits(val_idx=0, mabp=False):\n",
        "    SIGNAL_FOLDS_PATH = os.path.join(\"drive\", \"MyDrive\", \"datasets\", \"bcg_dataset\", \"signal_data\")\n",
        "\n",
        "    train_sp_data = train_dp_data = train_signal_data = None\n",
        "    eval_sp_data  = eval_dp_data  = eval_signal_data  = None\n",
        "\n",
        "    for file_name in os.listdir(SIGNAL_FOLDS_PATH):\n",
        "        if not file_name.endswith(\".mat\"):\n",
        "            continue\n",
        "        if (\"mabp\" in file_name) != mabp:\n",
        "            continue\n",
        "\n",
        "        is_eval = f\"fold_{val_idx}\" in file_name\n",
        "        raw = sio.loadmat(os.path.join(SIGNAL_FOLDS_PATH, file_name))\n",
        "        sp = torch.from_numpy(raw[\"SP\"]).float()\n",
        "        dp = torch.from_numpy(raw[\"DP\"]).float()\n",
        "        sig = torch.from_numpy(raw[\"signal\"]).float()\n",
        "\n",
        "        if is_eval:\n",
        "            eval_sp_data     = sp\n",
        "            eval_dp_data     = dp\n",
        "            eval_signal_data = sig\n",
        "        else:\n",
        "            train_sp_data     = sp  if train_sp_data is None else torch.vstack([train_sp_data, sp])\n",
        "            train_dp_data     = dp  if train_dp_data is None else torch.vstack([train_dp_data, dp])\n",
        "            train_signal_data = sig if train_signal_data is None else torch.vstack([train_signal_data, sig])\n",
        "\n",
        "    if train_sp_data is None or eval_sp_data is None:\n",
        "        raise ValueError(\"Missing training or evaluation data\")\n",
        "\n",
        "    train_data = torch.hstack([train_sp_data, train_dp_data, train_signal_data])\n",
        "    eval_data  = torch.hstack([eval_sp_data,  eval_dp_data,  eval_signal_data])\n",
        "    return train_data, eval_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PPGDataset(Dataset):\n",
        "    def __init__(self, data: torch.Tensor):\n",
        "        self.data = data\n",
        "        self.X = data[:, 2:]\n",
        "        self.y = data[:, :2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class PPGDatasetExtremeAug(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: torch.Tensor,\n",
        "        extreme_quantile: float = 0.05,\n",
        "        n_aug_per_extreme: int = 5\n",
        "    ):\n",
        "        self.data = data\n",
        "        self.X    = data[:, 2:]\n",
        "        self.y    = data[:, :2]\n",
        "        self.n_aug = n_aug_per_extreme\n",
        "        sp, dp = self.y[:,0], self.y[:,1]\n",
        "        sp_low, sp_high = torch.quantile(sp, extreme_quantile), torch.quantile(sp, 1-extreme_quantile)\n",
        "        dp_low, dp_high = torch.quantile(dp, extreme_quantile), torch.quantile(dp, 1-extreme_quantile)\n",
        "        sp_idxs = {i for i,v in enumerate(sp) if v <= sp_low or v >= sp_high}\n",
        "        dp_idxs = {i for i,v in enumerate(dp) if v <= dp_low or v >= dp_high}\n",
        "        self.extreme_idx = list(sp_idxs.union(dp_idxs))\n",
        "        all_idx = set(range(len(self.y)))\n",
        "        self.normal_idx = list(all_idx.difference(self.extreme_idx))\n",
        "        self.indices = []\n",
        "        self.indices += self.normal_idx\n",
        "        for i in self.extreme_idx:\n",
        "            self.indices.append(i)\n",
        "            for _ in range(self.n_aug):\n",
        "                self.indices.append((i, _))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.indices[idx]\n",
        "        if isinstance(key, int):\n",
        "            return self.X[key], self.y[key]\n",
        "        orig_i, _ = key\n",
        "        x = self.X[orig_i].clone()\n",
        "        y = self.y[orig_i]\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "pG1jvuZzRzPW"
      },
      "id": "pG1jvuZzRzPW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_steps: int,\n",
        "            in_channels: int,\n",
        "            out_channels: int,\n",
        "            stride: int,\n",
        "            config: dict\n",
        "    ):\n",
        "        super(Block, self).__init__()\n",
        "        self.n_steps = n_steps\n",
        "        self.spike_grad = config[\"spike_grad\"]\n",
        "\n",
        "        # --- main path ---\n",
        "        self.bn1   = nn.ModuleList([nn.BatchNorm1d(in_channels)   for _ in range(n_steps)])\n",
        "        self.lif1  = snn.Leaky(beta=0.95, learn_beta=True, learn_threshold=True, spike_grad=self.spike_grad)\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels,kernel_size=3, stride=stride, padding=1)\n",
        "\n",
        "        self.bn2   = nn.ModuleList([nn.BatchNorm1d(out_channels)  for _ in range(n_steps)])\n",
        "        self.lif2  = snn.Leaky(beta=0.95, learn_beta=True, learn_threshold=True, spike_grad=self.spike_grad)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # --- residual projection ---\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "    def forward(self, dati):\n",
        "        x_seq, mem1, mem2 = dati\n",
        "        out_seq = []\n",
        "\n",
        "        for t in range(self.n_steps):\n",
        "            x   = x_seq[t]\n",
        "            res = x if self.downsample is None else self.downsample(x)\n",
        "\n",
        "            out, mem1 = self.lif1(self.bn1[t](x), mem1)\n",
        "            out       = self.conv1(out)\n",
        "\n",
        "            out, mem2 = self.lif2(self.bn2[t](out), mem2)\n",
        "            out       = self.conv2(out)\n",
        "\n",
        "            # now out.shape == res.shape\n",
        "            out = out + res\n",
        "            out = self.dropout(out)\n",
        "            out_seq.append(out)\n",
        "\n",
        "        return out_seq, mem1, mem2"
      ],
      "metadata": {
        "id": "j4EDxLyOIpLf"
      },
      "id": "j4EDxLyOIpLf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhlUgnlxpG93",
      "metadata": {
        "id": "dhlUgnlxpG93"
      },
      "outputs": [],
      "source": [
        "class SpikingDABlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_steps: int,\n",
        "        in_channels_list: list[int],\n",
        "        out_channels: int,\n",
        "        downsample_strides: list[int],\n",
        "        config: dict\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_steps: number of time steps for spiking sequences\n",
        "            in_channels_list: channels of each input stream\n",
        "            out_channels: channels for the fused output\n",
        "            downsample_strides: temporal downsample factor per input (power of 2)\n",
        "        \"\"\"\n",
        "        super(SpikingDABlock, self).__init__()\n",
        "        assert len(in_channels_list) == len(downsample_strides), \\\n",
        "            \"in_channels_list and downsample_strides must match in length\"\n",
        "\n",
        "        self.n_steps    = n_steps\n",
        "        self.num_inputs = len(in_channels_list)\n",
        "        self.spike_grad = config[\"spike_grad\"]\n",
        "\n",
        "        self.transforms = nn.ModuleList()\n",
        "        self.bns        = nn.ModuleList()\n",
        "        self.lifs       = nn.ModuleList()\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # Build a 1x1 conv for each input with its own stride\n",
        "        for c_in, stride in zip(in_channels_list, downsample_strides):\n",
        "            self.transforms.append(nn.Conv1d(c_in, out_channels, kernel_size=1, stride=stride))\n",
        "            # time-step-specific batchnorm\n",
        "            self.bns.append(nn.ModuleList([nn.BatchNorm1d(out_channels) for _ in range(n_steps)]))\n",
        "            # one LIF per input stream\n",
        "            self.lifs.append(snn.Leaky(beta=0.95, learn_beta=True, spike_grad=self.spike_grad))\n",
        "\n",
        "    def forward(self, x_seq_list: list[list[torch.Tensor]], mem_list: list[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x_seq_list: list of length num_inputs, each a list of Tensors (len n_steps)\n",
        "            mem_list: list of length num_inputs, each initial membrane state\n",
        "        Returns:\n",
        "            out_seq: fused spike sequence (list of Tensors)\n",
        "            mem_list: updated membrane states\n",
        "        \"\"\"\n",
        "        N = len(x_seq_list)\n",
        "        out_seq = []\n",
        "\n",
        "        for t in range(self.n_steps):\n",
        "            sum_spk = 0\n",
        "            for i, seq in enumerate(x_seq_list):\n",
        "                x   = seq[t]\n",
        "                x   = self.transforms[i](x)\n",
        "                x   = self.bns[i][t](x)\n",
        "                spk, new_mem = self.lifs[i](x, mem_list[i])\n",
        "                sum_spk    += spk\n",
        "                mem_list[i] = new_mem\n",
        "\n",
        "            fused = sum_spk / N\n",
        "            fused = self.dropout(fused)\n",
        "            out_seq.append(fused)\n",
        "\n",
        "        return out_seq, mem_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uvaea585pKLT",
      "metadata": {
        "id": "uvaea585pKLT"
      },
      "outputs": [],
      "source": [
        "class SpikingResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_steps: int,\n",
        "        block: nn.Module,\n",
        "        dablock: nn.Module,\n",
        "        layers: list[int],\n",
        "        signal_channels: int,\n",
        "        num_classes: int,\n",
        "        config: dict\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_steps     = n_steps\n",
        "        self.in_channels = 64\n",
        "        self.block_cls   = block\n",
        "        self.dablock_cls = dablock\n",
        "\n",
        "        self.spike_grad = config[\"spike_grad\"]\n",
        "        self.encoding = config[\"encoding\"]\n",
        "\n",
        "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        # Stem\n",
        "        beta0 = torch.rand(313)\n",
        "\n",
        "        self.conv0 = nn.Conv1d(signal_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bns0   = nn.ModuleList([nn.BatchNorm1d(64) for _ in range(self.n_steps)])\n",
        "        self.lif0  = snn.Leaky(beta=beta0, learn_beta=True, learn_threshold=True, spike_grad=self.spike_grad)\n",
        "\n",
        "        # Residual stages\n",
        "        self.stage1 = self._make_stage(layers[0],  64, stride=1, config=config)\n",
        "        self.stage2 = self._make_stage(layers[1],  128, stride=2, config=config)\n",
        "        self.stage3 = self._make_stage(layers[2], 256, stride=2, config=config)\n",
        "        self.stage4 = self._make_stage(layers[3], 512, stride=2, config=config)\n",
        "\n",
        "        # DA fusion blocks with downsample_strides for each input\n",
        "        self.da12   = self.dablock_cls(n_steps, [64, 64],                64, downsample_strides=[1, 1],          config=config)\n",
        "        self.da123  = self.dablock_cls(n_steps, [64, 64, 128],            128, downsample_strides=[2, 2, 1],       config=config)\n",
        "        self.da1234 = self.dablock_cls(n_steps, [64, 64, 128, 256],      256, downsample_strides=[4, 4, 2, 1],    config=config)\n",
        "        self.da_all = self.dablock_cls(n_steps, [64, 64, 128, 256, 512], 512, downsample_strides=[8, 8, 4, 2, 1], config=config)\n",
        "\n",
        "        # Classifier head\n",
        "        beta_out_1   = torch.rand(1)\n",
        "        thr_out_1    = torch.rand(1)\n",
        "        beta_out_2   = torch.rand(1)\n",
        "        thr_out_2    = torch.rand(1)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout  = nn.Dropout(config[\"dropout\"])\n",
        "\n",
        "        self.fc_out_1   = nn.Linear(512, 1)\n",
        "        self.lif_out_1 = snn.Leaky(beta=beta_out_1, threshold=thr_out_1, learn_beta=True, learn_threshold=True, spike_grad=self.spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "        self.fc_out_2   = nn.Linear(512, 1)\n",
        "        self.lif_out_2 = snn.Leaky(beta=beta_out_2, threshold=thr_out_2, learn_beta=True, learn_threshold=True, spike_grad=self.spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # helper to reset membrane states\n",
        "        def reset(lif): return lif.reset_mem()\n",
        "\n",
        "        # reset & store\n",
        "        mem_stem    = reset(self.lif0)\n",
        "        mem_stage1, mem_stage2, mem_stage3, mem_stage4 = [\n",
        "            [[reset(blk.lif1), reset(blk.lif2)] for blk in stage]\n",
        "            for stage in (self.stage1, self.stage2, self.stage3, self.stage4)\n",
        "        ]\n",
        "        mem_da12   = [reset(l) for l in self.da12.lifs]\n",
        "        mem_da123  = [reset(l) for l in self.da123.lifs]\n",
        "        mem_da1234 = [reset(l) for l in self.da1234.lifs]\n",
        "        mem_da_all = [reset(l) for l in self.da_all.lifs]\n",
        "\n",
        "        mem_cls_1  = reset(self.lif_out_1)\n",
        "        mem_cls_2  = reset(self.lif_out_2)\n",
        "\n",
        "        # f0 / stem\n",
        "        stem_seq  = []\n",
        "\n",
        "        for t in range(self.n_steps):\n",
        "            if self.encoding:\n",
        "                x_step = x[t]\n",
        "            else:\n",
        "                x_step = x\n",
        "\n",
        "            cur_stem = self.bns0[t](self.conv0(x_step))\n",
        "            spk, mem_stem = self.lif0(cur_stem, mem_stem)\n",
        "            stem_seq.append(spk)\n",
        "\n",
        "        # stage 1\n",
        "        seq = stem_seq\n",
        "        for idx, blk in enumerate(self.stage1):\n",
        "            m1, m2 = mem_stage1[idx]\n",
        "            seq, m1, m2 = blk([seq, m1, m2])\n",
        "            mem_stage1[idx] = [m1, m2]\n",
        "        out_seq1_da, mem_da12 = self.da12([stem_seq, seq], mem_da12)\n",
        "        out_seq1p = [f0 + d for f0, d in zip(stem_seq, out_seq1_da)]\n",
        "\n",
        "        # stage 2\n",
        "        seq = out_seq1p\n",
        "        for idx, blk in enumerate(self.stage2):\n",
        "            m1, m2 = mem_stage2[idx]\n",
        "            seq, m1, m2 = blk([seq, m1, m2])\n",
        "            mem_stage2[idx] = [m1, m2]\n",
        "\n",
        "        out_seq2_da, mem_da123 = self.da123([stem_seq, out_seq1p, seq], mem_da123)\n",
        "        out_seq2p = [f2 + d for f2, d in zip(seq, out_seq2_da)]\n",
        "\n",
        "        # stage 3\n",
        "        seq = out_seq2p\n",
        "        for idx, blk in enumerate(self.stage3):\n",
        "            m1, m2 = mem_stage3[idx]\n",
        "            seq, m1, m2 = blk([seq, m1, m2])\n",
        "            mem_stage3[idx] = [m1, m2]\n",
        "        out_seq3_da, mem_da1234 = self.da1234([\n",
        "            stem_seq, out_seq1p, out_seq2p, seq\n",
        "        ], mem_da1234)\n",
        "        out_seq3p = [f3 + d for f3, d in zip(seq, out_seq3_da)]\n",
        "\n",
        "        # stage 4\n",
        "        seq = out_seq3p\n",
        "        for idx, blk in enumerate(self.stage4):\n",
        "            m1, m2 = mem_stage4[idx]\n",
        "            seq, m1, m2 = blk([seq, m1, m2])\n",
        "            mem_stage4[idx] = [m1, m2]\n",
        "        out_seq4_da, mem_da_all = self.da_all([\n",
        "            stem_seq, out_seq1p, out_seq2p, out_seq3p, seq\n",
        "        ], mem_da_all)\n",
        "        out_seq4p = [f4 + d for f4, d in zip(seq, out_seq4_da)]\n",
        "\n",
        "        # classifier head\n",
        "        mem_seq_1 = []\n",
        "        mem_seq_2 = []\n",
        "        for t in range(self.n_steps):\n",
        "\n",
        "            pooled = self.avg_pool(out_seq4p[t]).squeeze(-1)\n",
        "            pooled = self.dropout(pooled)\n",
        "\n",
        "            cur_cls_1 = self.fc_out_1(pooled)\n",
        "            spk_1, mem_cls_1 = self.lif_out_1(cur_cls_1, mem_cls_1)\n",
        "\n",
        "            cur_cls_2 = self.fc_out_2(pooled)\n",
        "            spk_2, mem_cls_2 = self.lif_out_2(cur_cls_2, mem_cls_2)\n",
        "\n",
        "            mem_seq_1.append(mem_cls_1)\n",
        "            mem_seq_2.append(mem_cls_2)\n",
        "\n",
        "        mem_seq_1 = torch.stack(mem_seq_1, dim=0)\n",
        "        mem_seq_2 = torch.stack(mem_seq_2, dim=0)\n",
        "\n",
        "        mem_seq = torch.cat([mem_seq_1, mem_seq_2], dim=2)\n",
        "\n",
        "        # return the list of final membrane states across time\n",
        "        return mem_seq\n",
        "\n",
        "    def _make_stage(self, num_blocks, out_channels, stride, config):\n",
        "        blocks = []\n",
        "        for i in range(num_blocks):\n",
        "            blk_stride = stride if i == 0 else 1\n",
        "\n",
        "            blocks.append(self.block_cls(self.n_steps, self.in_channels, out_channels, blk_stride, config))\n",
        "\n",
        "            self.in_channels = out_channels\n",
        "\n",
        "        return nn.ModuleList(blocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "806d6baa",
      "metadata": {
        "id": "806d6baa"
      },
      "outputs": [],
      "source": [
        "MODEL_CONFIG = {\n",
        "    \"num_steps\": 24,\n",
        "    \"spike_grad\": surrogate.atan(alpha=8),\n",
        "    \"encoding\": True,\n",
        "    \"layers\": [1, 1, 1, 1],\n",
        "    \"dropout\": 0.3,\n",
        "}\n",
        "\n",
        "TRAIN_CONFIG = {\n",
        "    \"batch_size\":         256,\n",
        "    \"n_epochs\":           100,\n",
        "    \"lr\":                 5e-4,\n",
        "    \"weight_decay\":       5e-5,\n",
        "    \"feature_scaling\":    True,\n",
        "    \"target_scaling\":     True,\n",
        "    \"device\":             torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    # augmentation & oversampling params\n",
        "    \"extreme_quantile\":   0.05,\n",
        "    \"n_aug_per_extreme\":  3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Idvhho5234Dn",
      "metadata": {
        "id": "Idvhho5234Dn"
      },
      "outputs": [],
      "source": [
        "def scale_data(train_data, eval_data):\n",
        "    if TRAIN_CONFIG[\"feature_scaling\"]:\n",
        "        fs = StandardScaler().fit(train_data[:,2:])\n",
        "        train_data[:,2:] = torch.from_numpy(fs.transform(train_data[:,2:])).float()\n",
        "        eval_data[:,2:]  = torch.from_numpy(fs.transform(eval_data[:,2:])).float()\n",
        "        if MODEL_CONFIG[\"encoding\"]:\n",
        "            mms = MinMaxScaler((0,1)).fit(train_data[:,2:])\n",
        "            train_data[:,2:] = torch.from_numpy(mms.transform(train_data[:,2:])).float()\n",
        "            eval_data[:,2:]  = torch.from_numpy(mms.transform(eval_data[:,2:])).float()\n",
        "    if TRAIN_CONFIG[\"target_scaling\"]:\n",
        "        ts = StandardScaler().fit(train_data[:,:2])\n",
        "        train_data[:,:2] = torch.from_numpy(ts.transform(train_data[:,:2])).float()\n",
        "        eval_data[:,:2]  = torch.from_numpy(ts.transform(eval_data[:,:2])).float()\n",
        "    else:\n",
        "        ts = None\n",
        "    return train_data, eval_data, ts\n",
        "\n",
        "def make_dataloaders(train_data, eval_data):\n",
        "    # train_ds = PPGDatasetExtremeAug(\n",
        "    #     data=train_data,\n",
        "    #     extreme_quantile=TRAIN_CONFIG[\"extreme_quantile\"],\n",
        "    #     n_aug_per_extreme=TRAIN_CONFIG[\"n_aug_per_extreme\"]\n",
        "    # )\n",
        "    train_ds = PPGDataset(train_data)\n",
        "    val_ds   = PPGDataset(eval_data)\n",
        "    train_loader = DataLoader(train_ds, batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
        "                              shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
        "                              shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Model\n",
        "def build_model():\n",
        "    return SpikingResNet(\n",
        "        n_steps=MODEL_CONFIG[\"num_steps\"],\n",
        "        layers=MODEL_CONFIG[\"layers\"],\n",
        "        block=Block,\n",
        "        dablock=SpikingDABlock,\n",
        "        signal_channels=1,\n",
        "        num_classes=2,\n",
        "        config=MODEL_CONFIG\n",
        "    ).to(TRAIN_CONFIG[\"device\"])\n",
        "\n",
        "# Training / Validation Epochs\n",
        "def train_one_epoch(model, loader, loss_fn, optimizer, scheduler, targ_scl):\n",
        "    model.train()\n",
        "    total_loss = sp_mae = dp_mae = 0.0\n",
        "    for Xb, yb in loader:\n",
        "        X = Xb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "        y = yb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "        if MODEL_CONFIG[\"encoding\"]:\n",
        "            X = spikegen.rate(data=X.unsqueeze(1), num_steps=MODEL_CONFIG[\"num_steps\"])\n",
        "        else:\n",
        "            X = X.unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        yp = model(X)[-1]\n",
        "        loss = loss_fn(yp, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        bsz = X.size(0)\n",
        "        total_loss += loss.item() * bsz\n",
        "        if targ_scl:\n",
        "            yt = torch.from_numpy(targ_scl.inverse_transform(y.cpu().numpy())).to(y.device)\n",
        "            yh = torch.from_numpy(targ_scl.inverse_transform(yp.detach().cpu().numpy())).to(y.device)\n",
        "        else:\n",
        "            yt, yh = y, yp\n",
        "        sp_mae += torch.abs(yh[:,0] - yt[:,0]).sum().item()\n",
        "        dp_mae += torch.abs(yh[:,1] - yt[:,1]).sum().item()\n",
        "    n = len(loader.dataset)\n",
        "    return total_loss/n, sp_mae/n, dp_mae/n\n",
        "\n",
        "def validate_one_epoch(model, loader, loss_fn, targ_scl):\n",
        "    model.eval()\n",
        "    total_loss = sp_mae = dp_mae = 0.0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            X = Xb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "            y = yb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "            if MODEL_CONFIG[\"encoding\"]:\n",
        "                X = spikegen.rate(data=X.unsqueeze(1), num_steps=MODEL_CONFIG[\"num_steps\"])\n",
        "            else:\n",
        "                X = X.unsqueeze(1)\n",
        "            yp = model(X)[-1]\n",
        "            bsz = X.size(0)\n",
        "            total_loss += loss_fn(yp, y).item() * bsz\n",
        "            if targ_scl:\n",
        "                yt = torch.from_numpy(targ_scl.inverse_transform(y.cpu().numpy())).to(y.device)\n",
        "                yh = torch.from_numpy(targ_scl.inverse_transform(yp.cpu().numpy())).to(y.device)\n",
        "            else:\n",
        "                yt, yh = y, yp\n",
        "            sp_mae += torch.abs(yh[:,0] - yt[:,0]).sum().item()\n",
        "            dp_mae += torch.abs(yh[:,1] - yt[:,1]).sum().item()\n",
        "    n = len(loader.dataset)\n",
        "    return total_loss/n, sp_mae/n, dp_mae/n\n",
        "\n",
        "def predict_on_loader(model, loader, targ_scl=None):\n",
        "    model.eval()\n",
        "    trues, preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in loader:\n",
        "            X = Xb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "            y = yb.to(TRAIN_CONFIG[\"device\"]).float()\n",
        "            if MODEL_CONFIG[\"encoding\"]:\n",
        "                X = spikegen.rate(data=X.unsqueeze(1), num_steps=MODEL_CONFIG[\"num_steps\"])\n",
        "            else:\n",
        "                X = X.unsqueeze(1)\n",
        "            yp = model(X)[-1]\n",
        "\n",
        "            if targ_scl:\n",
        "                yt = targ_scl.inverse_transform(y.cpu().numpy())\n",
        "                yh = targ_scl.inverse_transform(yp.cpu().numpy())\n",
        "            else:\n",
        "                yt, yh = y.cpu().numpy(), yp.cpu().numpy()\n",
        "\n",
        "            trues.append(yt)\n",
        "            preds.append(yh)\n",
        "\n",
        "    return np.vstack(trues), np.vstack(preds)\n",
        "\n",
        "# Fold / K-Fold Runner\n",
        "def run_fold(val_idx, fold_num=None):\n",
        "    if fold_num is not None:\n",
        "        print(f\"\\n=== Starting fold {fold_num} ===\")\n",
        "\n",
        "    train_data, eval_data = get_data_splits(val_idx=val_idx, mabp=False)\n",
        "    train_data, eval_data, targ_scl = scale_data(train_data, eval_data)\n",
        "    train_loader, val_loader = make_dataloaders(train_data, eval_data)\n",
        "\n",
        "    model     = build_model()\n",
        "    loss_fn   = nn.L1Loss()\n",
        "\n",
        "    max_lr  = TRAIN_CONFIG[\"lr\"]\n",
        "    base_lr = max_lr / 50.0\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=TRAIN_CONFIG[\"weight_decay\"])\n",
        "\n",
        "    total_steps = TRAIN_CONFIG[\"n_epochs\"] * len(train_loader)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=max_lr,\n",
        "        total_steps=total_steps,\n",
        "        div_factor=50,\n",
        "        pct_start=0.3,\n",
        "        final_div_factor=1e3\n",
        "        )\n",
        "\n",
        "    # best_val, no_imp = float('inf'), 0\n",
        "    # patience, delta = 15, 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_sps, val_sps     = [], []\n",
        "    train_dps, val_dps     = [], []\n",
        "    for ep in range(1, TRAIN_CONFIG[\"n_epochs\"]+1):\n",
        "        tr_loss, tr_sp, tr_dp = train_one_epoch(model, train_loader, loss_fn, optimizer, scheduler, targ_scl)\n",
        "        vl_loss, vl_sp, vl_dp = validate_one_epoch(model, val_loader, loss_fn, targ_scl)\n",
        "\n",
        "        if ep % 2 == 0:\n",
        "            print(f\"Fold {fold_num} Epoch {ep}/{TRAIN_CONFIG['n_epochs']}  Train L={tr_loss:.4f} (SP={tr_sp:.3f}, DP={tr_dp:.3f})  Val  L={vl_loss:.4f} (SP={vl_sp:.3f}, DP={vl_dp:.3f})\")\n",
        "        train_losses.append(tr_loss)\n",
        "        val_losses.append(vl_loss)\n",
        "        train_sps.append(tr_sp)\n",
        "        val_sps.append(vl_sp)\n",
        "        train_dps.append(tr_dp)\n",
        "        val_dps.append(vl_dp)\n",
        "        #if vl_loss < best_val - delta:\n",
        "        #    best_val, no_imp = vl_loss, 0\n",
        "        #else:\n",
        "        #    no_imp += 1\n",
        "        #    if no_imp >= patience:\n",
        "        #        print(f\"Fold {fold_num} stopping early at epoch {ep}\")\n",
        "        #        torch.save(model.state_dict(), f\"best_model_fold{fold_num}.pt\")\n",
        "        #        break\n",
        "\n",
        "    torch.save(model.state_dict(), f\"best_model_fold{fold_num}.pt\")\n",
        "    y_trues, y_preds = predict_on_loader(model, val_loader, targ_scl=targ_scl)\n",
        "\n",
        "    metrics = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_loss\":   val_losses,\n",
        "        \"train_sp\":   train_sps,\n",
        "        \"val_sp\":     val_sps,\n",
        "        \"train_dp\":   train_dps,\n",
        "        \"val_dp\":     val_dps,\n",
        "    }\n",
        "\n",
        "    return vl_sp, vl_dp, metrics, y_trues, y_preds\n",
        "\n",
        "def run_kfold(n_folds=5):\n",
        "    sbps, dbps, logs = [], [], {}\n",
        "    all_trues, all_preds = [], []\n",
        "\n",
        "    for f in range(n_folds):\n",
        "        sp_mae, dp_mae, lg, y_trues, y_preds = run_fold(val_idx=f, fold_num=f+1 )\n",
        "        print(f\"→ Fold {f+1} result: \"\n",
        "        f\"SBP MAE={sp_mae:.3f}, DBP MAE={dp_mae:.3f}\")\n",
        "\n",
        "        sbps.append(sp_mae)\n",
        "        dbps.append(dp_mae)\n",
        "        logs[f] = lg\n",
        "\n",
        "        # collect for the final pooled analysis\n",
        "        all_trues.append(y_trues)\n",
        "        all_preds.append(y_preds)\n",
        "\n",
        "    # compute mean ± std\n",
        "    mean_sbp, mean_dbp = np.mean(sbps), np.mean(dbps)\n",
        "    std_sbp,  std_dbp  = np.std(sbps, ddof=1), np.std(dbps, ddof=1)\n",
        "\n",
        "    print(f\"\\n=== K-Fold Summary ({n_folds} folds) ===\")\n",
        "    print(f\"SBP MAE = {mean_sbp:.3f} ± {std_sbp:.3f}  •  \"\n",
        "    f\"DBP MAE = {mean_dbp:.3f} ± {std_dbp:.3f}\")\n",
        "\n",
        "    # stack up all folds’ val predictions & truths\n",
        "    y_true_all = np.vstack(all_trues)\n",
        "    y_pred_all = np.vstack(all_preds)\n",
        "\n",
        "    with open(\"learning_curves.json\", \"w\") as f:\n",
        "        json.dump(logs, f, indent=2)\n",
        "\n",
        "    return mean_sbp, std_sbp, mean_dbp, std_dbp, y_true_all, y_pred_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gaugsRAq6psk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "gaugsRAq6psk",
        "outputId": "54f8cbee-b3fd-47f6-edcf-ebd1ad0cbc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Starting fold 1 ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 7.38 MiB is free. Process 6779 has 22.15 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 5.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-efba8ed21fde>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_sbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_sbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_dbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_dbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overall SBP MAE = {mean_sbp:.3f} ± {std_sbp:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overall DBP MAE = {mean_dbp:.3f} ± {std_dbp:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9667b845629a>\u001b[0m in \u001b[0;36mrun_kfold\u001b[0;34m(n_folds)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0msp_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         print(f\"→ Fold {f+1} result: \"\n\u001b[1;32m    194\u001b[0m         f\"SBP MAE={sp_mae:.3f}, DBP MAE={dp_mae:.3f}\")\n",
            "\u001b[0;32m<ipython-input-11-9667b845629a>\u001b[0m in \u001b[0;36mrun_fold\u001b[0;34m(val_idx, fold_num)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mtrain_dps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dps\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_CONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_scl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mvl_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_sp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvl_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_scl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9667b845629a>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, loss_fn, optimizer, scheduler, targ_scl)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0myp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-12a6b0e05f78>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem_stage3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mmem_stage3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         out_seq3_da, mem_da1234 = self.da1234([\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-9cc8480e9d5c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dati)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlif1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mout\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snntorch/_neurons/leaky.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_, mem)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snntorch/_neurons/neurons.py\u001b[0m in \u001b[0;36mmem_reset\u001b[0;34m(self, mem)\u001b[0m\n\u001b[1;32m    104\u001b[0m         Returns reset.\"\"\"\n\u001b[1;32m    105\u001b[0m         \u001b[0mmem_shift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspike_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_shift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snntorch/surrogate.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mATan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/snntorch/surrogate.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input_, alpha)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 7.38 MiB is free. Process 6779 has 22.15 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 5.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "mean_sbp, std_sbp, mean_dbp, std_dbp, y_true_all, y_pred_all = run_kfold(n_folds=5)\n",
        "print(f\"Overall SBP MAE = {mean_sbp:.3f} ± {std_sbp:.3f}\")\n",
        "print(f\"Overall DBP MAE = {mean_dbp:.3f} ± {std_dbp:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "with open(\"learning_curves.json\", \"r\") as f:\n",
        "    logs = json.load(f)\n",
        "\n",
        "# Plot settings\n",
        "metrics = {\n",
        "    \"loss\": (\"Loss\", \"Training and Validation Loss\"),\n",
        "    \"sp\": (\"SBP MAE\", \"SBP Mean Absolute Error\"),\n",
        "    \"dp\": (\"DBP MAE\", \"DBP Mean Absolute Error\")\n",
        "}\n",
        "\n",
        "# Plot each metric with one subplot per fold\n",
        "for key, (ylabel, title) in metrics.items():\n",
        "    n_folds = len(logs)\n",
        "    fig, axs = plt.subplots(1, n_folds, figsize=(5 * n_folds, 4), sharey=True)\n",
        "    fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    if n_folds == 1:\n",
        "        axs = [axs]  # ensure it's iterable\n",
        "\n",
        "    for i, (fold, data) in enumerate(logs.items()):\n",
        "        axs[i].plot(data[f\"train_{key}\"], label=\"Train\", color=\"blue\", linewidth=2)\n",
        "        axs[i].plot(data[f\"val_{key}\"], label=\"Val\", color=\"orange\", linestyle=\"--\", linewidth=2)\n",
        "        axs[i].set_title(f\"Fold {fold}\")\n",
        "        axs[i].set_xlabel(\"Epoch\")\n",
        "        axs[i].set_ylabel(ylabel)\n",
        "        axs[i].legend()\n",
        "        axs[i].grid(True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "_rzRmYFPGjEW"
      },
      "id": "_rzRmYFPGjEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def plot_error_histograms(y_true, y_pred):\n",
        "    \"\"\"Plot side-by-side histograms of absolute errors for SBP & DBP.\"\"\"\n",
        "    errors = np.abs(y_pred - y_true)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    axes[0].hist(errors[:, 0], bins=50, alpha=0.7)\n",
        "    axes[0].set_title(\"SBP Absolute Error Histogram\")\n",
        "    axes[0].set_xlabel(\"Absolute Error (mmHg)\")\n",
        "    axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "    axes[1].hist(errors[:, 1], bins=50, alpha=0.7)\n",
        "    axes[1].set_title(\"DBP Absolute Error Histogram\")\n",
        "    axes[1].set_xlabel(\"Absolute Error (mmHg)\")\n",
        "    axes[1].set_ylabel(\"Count\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bland_altman(y_true, y_pred):\n",
        "    \"\"\"Bland–Altman plot: prediction error vs. average of true & pred.\"\"\"\n",
        "    errors = y_pred - y_true\n",
        "    averages = (y_pred + y_true) / 2\n",
        "\n",
        "    for i, label in enumerate([\"SBP\", \"DBP\"]):\n",
        "        avg = averages[:, i]\n",
        "        err = errors[:, i]\n",
        "        m = np.mean(err)\n",
        "        sd = np.std(err, ddof=1)\n",
        "\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.scatter(avg, err, alpha=0.5)\n",
        "        plt.axhline(m, linestyle='-', label=f\"Mean Error = {m:.2f}\")\n",
        "        plt.axhline(m + 1.96*sd, linestyle='--', label=f\"+1.96 SD = {m + 1.96*sd:.2f}\")\n",
        "        plt.axhline(m - 1.96*sd, linestyle='--', label=f\"-1.96 SD = {m - 1.96*sd:.2f}\")\n",
        "        plt.title(f\"Bland–Altman Plot ({label})\")\n",
        "        plt.xlabel(\"Average of True & Predicted (mmHg)\")\n",
        "        plt.ylabel(\"Error (Predicted − True) (mmHg)\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_scatter_with_r2(y_true, y_pred):\n",
        "    \"\"\"Scatter plot of predicted vs. true values, plus identity line & R².\"\"\"\n",
        "    for i, label in enumerate([\"SBP\", \"DBP\"]):\n",
        "        true = y_true[:, i]\n",
        "        pred = y_pred[:, i]\n",
        "        r2 = r2_score(true, pred)\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.scatter(true, pred, alpha=0.5)\n",
        "        lims = [min(true.min(), pred.min()), max(true.max(), pred.max())]\n",
        "        plt.plot(lims, lims, 'k--', linewidth=1)\n",
        "        plt.xlim(lims)\n",
        "        plt.ylim(lims)\n",
        "        plt.title(f\"{label}: Predicted vs True (R² = {r2:.3f})\")\n",
        "        plt.xlabel(\"True (mmHg)\")\n",
        "        plt.ylabel(\"Predicted (mmHg)\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def plot_value_distributions(y_true, y_pred, bins=100):\n",
        "    \"\"\"\n",
        "    Plot side-by-side histograms of the TRUE and PREDICTED values for SBP & DBP.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    for i, label in enumerate([\"SBP\", \"DBP\"]):\n",
        "        axes[i].hist(y_true[:, i], bins=bins, alpha=0.6, label=\"True\", edgecolor='black')\n",
        "        axes[i].hist(y_pred[:, i], bins=bins, alpha=0.6, label=\"Predicted\", edgecolor='black')\n",
        "        axes[i].set_title(f\"{label} Value Distribution\")\n",
        "        axes[i].set_xlabel(\"Blood Pressure (mmHg)\")\n",
        "        axes[i].set_ylabel(\"Count\")\n",
        "        axes[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k-L16h1HmDu9"
      },
      "id": "k-L16h1HmDu9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_error_histograms(y_true_all, y_pred_all)\n",
        "plot_bland_altman(y_true_all, y_pred_all)\n",
        "plot_scatter_with_r2(y_true_all, y_pred_all)\n",
        "plot_value_distributions(y_true_all, y_pred_all)"
      ],
      "metadata": {
        "id": "ioCb_QbMmHJf"
      },
      "id": "ioCb_QbMmHJf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FRPAootYuK4Y",
      "metadata": {
        "id": "FRPAootYuK4Y"
      },
      "outputs": [],
      "source": [
        "def make_serializable(d):\n",
        "    serial = {}\n",
        "    for k, v in d.items():\n",
        "        # torch.device → string\n",
        "        if isinstance(v, torch.device):\n",
        "            serial[k] = str(v)\n",
        "        # functions or snntorch surrogate objs → use repr()\n",
        "        elif callable(v) or not isinstance(v, (int, float, bool, str, list, dict)):\n",
        "            serial[k] = repr(v)\n",
        "        else:\n",
        "            serial[k] = v\n",
        "    return serial\n",
        "\n",
        "model_cfg_ser = make_serializable(MODEL_CONFIG)\n",
        "train_cfg_ser = make_serializable(TRAIN_CONFIG)\n",
        "\n",
        "with open(\"final_metrics.json\",\"a\") as f:\n",
        "    f.write(f\"\\nFinal K-Fold SBP MAE: {mean_sbp:.4f}, DBP MAE: {mean_dbp:.4f}\\n\")\n",
        "    f.write(\"\\nModel Configuration:\\n\")\n",
        "    json.dump(model_cfg_ser, f, indent=2); f.write(\"\\n\")\n",
        "    f.write(\"\\nTraining Configuration:\\n\")\n",
        "    json.dump(train_cfg_ser, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E0zmjJriK6Dz"
      },
      "id": "E0zmjJriK6Dz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}